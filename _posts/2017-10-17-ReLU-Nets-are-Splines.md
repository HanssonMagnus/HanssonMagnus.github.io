---
layout: post
title: ReLU nets are splines
---

### Introduction

Artificial Neural Networks (ANN or just NN) has been increasingly popular during the last years. With API's such as TensorFlow deep learning is easily accessible not only for the machine learning experts. This leads to a lot of practice and hands on experience but it might also lead to a lack of theoretical knowledge about the approximation methods.

In this post I will try to shed some light upon how a neural network with the increasingly popular ReLU activation function actually makes its approximations.

### ReLU nets are splines

$x^2 + y^2$ $$x^2 + y^2$$
